\section{Mesh Fitting}
\label{sec:fitting}

We pose mesh fitting to 3D point data as finding the most likely surface that would have generated those points.  By incorporating prior surface assumptions, the fitting process estimates a continuous surface from discrete points that can eliminate much of the measurement noise.  Methods that fit mesh models to 3D points often minimize the perpendicular distance of points to facets [REF].  This makes sense when point-cloud noise is equal in all directions or else the point noise is small compared to the facets.  For our data the measurement noise is large and is not equal in all directions, but rather is along the depth camera rays.  Hence the focus of this section is to develop a mesh fitting method that minimizes these pixel depth errors along the pixel rays.

In this paper we define a mesh in a $2$D image space and project it into $3$D.  This is more limiting than full $3$D meshes as it models only the surface portions visible from the sensor, but it also provides a number of advantages.  Compared to methods that fit prior surface models to depth maps [ref], need to search of the space of poses, scales and distortions of the model with the chance of finding local minima.  Compared to voxel-based models with implicit surfaces [ref], our method can better incorporate pixels uncertainties and surface priors, as well as having fewer discretization artifacts.  In addition our method can naturally incorporate detailed features from the high-resolution color camera, and reflectivity information from the IR reflectance image.

\subsection{Notation}

A vertex, $\vertex_j$, is a vector in $3$D.  In a given camera coordinate system, it projects onto a pixel on the unit focal-length image-plane $\hvertex_j=(u,v,1)^\top$, where the `` $\tilde{\text{ }}$ '' indicates a homogeneous vector, and $u$ and $v$ are the coordinates in this plane.  Now $\hvertex_j$ defines a ray from the camera origin, and the original vertex is obtained by scaling the image-plane vertex by its depth, $\lambda_j$, along the ray, namely: $\vertex_j = \lambda_j\hvertex_j$. 

\subsection{Facet Model}

Mesh fitting for an individual facet is illustrated in Figure~\ref{fig:facet}.  The $2$D vertices and edge connections are determined in an image, in this case the color image although it could be the depth image, as described in section~\ref{sec:colormesh}.  If these vertices lie on a feature of the target leaf, such as its edge, we know that that those $3$D features like somewhere along the rays emanating from camera origin through those vertices.  Hence a triangular facet approximation to the object surface will have vertices on these three rays.  

\begin{figure}
\begin{center}
   \includegraphics[trim=80 70 70 20,clip,width=0.95\linewidth]{Figures/pointFittingConcept}
\end{center}
   \caption{The parallel and adjacent color and depth cameras are shown as pyramids denoting their fields of view, and their size difference illustrates their relative resolutions.  Three vertices in a color image define the rays on which the vertices of the corresponding $3$D object facet must lie.  This facet is fit using the the $3$D points projected out from the depth camera.}
\label{fig:facet}
\end{figure}

The next step is to associate depth measurements with the facet.  Pixels in the depth camera are projected along their rays out into $3$D, and then they are projected into the color image.  We associate the depth pixel, $\point_i$, with the facet, $\mathcal{F}_i$, into which it projects in the color image, as illustrated in Figure~\ref{fig:facet}. 

To estimate the facet parameters from depth measurements we will express the depth points, $\point_i$, after they have been projected out and transformed into color camera coordinates, as a linear function of the vertices of its facet:
\beq  %see PlantMacros.tex
\point_i = \sum_{j\in\mathcal{F}_i} \alpha_j \vertex_j. \label{eq:point}
\eeq
Here $\mathcal{F}$ is the set of three vertex indices belonging to the facet, and $\alpha_j$ is the coefficient of vertex $\vertex_j$ as illustrated in Figure~\ref{fig:triangle}, and $\sum_{j\in\mathcal{F}}\alpha_j=1$.  This linear sum is valid if we make a local orthographic approximation for the projection of a facet.  It will be a good approximation as long as the facet size is small compared to is depth from the camera, which is true for most applications.  Substituting in depth-scaled homogeneous vectors, and taking the third row, we obtain an equation for the point depth, $\lambda_i$, in the color image:
\beq
\lambda_i = \sum_{j\in\mathcal{F}_i} \alpha_j \lambda_j. \label{eq:pointdepth}
\eeq

\begin{figure}
\begin{center}
   \includegraphics[trim=150 140 140 80,clip,width=0.75\linewidth]{Figures/TriangleParameterization}
\end{center}
   \caption{The coordinates of a point on a facet described by Eq.~(\ref{eq:point}) are the weighted linear sum of the three vertex coordinates.  The weight, $\alpha_a$, for vertex $\vertex_a$ is given by $\alpha_a = \frac{l_a}{h_a}$, the ratio of its perpendicular distance $l_a$ to the opposite edge to the vertex perpendicular distance $h_a$.  Analogous expressions describe $\alpha_b$ and $\alpha_c$. }
\label{fig:triangle}
\end{figure}

\subsection{Least Squares Depth}

Equation (\ref{eq:pointdepth}) gives the modeled depth of a point in terms of its facet vertices.  The depth camera will provide measured depths for each pixel, indicated as $\bar{\lambda}_i$, along with a standard deviation estimate $\sigma_i$ obtained from Eq.~(\ref{eq:sigma}).  Now this noise is along the depth ray, rather than the color camera ray, but since the depth and color cameras are close together compared to the distance to target, these rays are close to parallel and we assume $\sigma_i$ is a good measure along the camera ray.  With this approximation, the weighted least squares cost for between measured and predicted depth is:
\beq
E_{depth}(\Lambda_{\vertex}) = \sum_{i\in\mathcal{D}} \|  \bar{\lambda}_i -  \sum_{j\in\mathcal{F}_i} \alpha_j \lambda_j \|^2_{\sigma_i^2}, \label{eq:meshleastsquares}
\eeq
where $\mathcal{D}$ is the set of depth pixels that project onto the target.  The norm is weighted with the inverse variance of each point.  Minimizing this for $\Lambda_{\vertex}$, the set of all vertex depths, is a straightforward linear calculation.  It estimates the mesh and it correctly minimizes the measurement error.

\subsection{Shape from Shading}

The depth camera measures IR reflectance in addition to depth at each pixel.  Since the reflectance depends on the angle between the surface normal and the incident ray from the IR illuminator, the reflectance image can provide useful cues on the object surface.  Shape from shading techniques model this dependence on the surface normal, along with additional surface assumptions such as smoothness, to estimate the normals and integrate an object surface [REF].  However the real world practicality of these methods has been limited since they generally require a single known light source position illuminating a Lambertian surface, the integration is sensitive to noise, and shape is obtained only up to a scale factor.  Fortunately our application satisfies the key requirements of Shape from Shading, (we have a known light source and leafs are modeled well as Lambertian surfaces~\cite{Chelle2006219}), and our mesh model provides additional information that removes the need to integrate noisy surface normals.  This section describes our use of Shape from Shading to improve shape recovery.

\begin{figure}
\begin{center}
   \includegraphics[trim=100 100 100 40,clip,width=0.95\linewidth]{Figures/ShapeFromShading}
\end{center}
   \caption{Our geometric model for the reflectance image.  At each depth point the intensity of the reflectance image will depend on a number of factors including the angle between the facet normal and the ray to the illuminator shown for two points as $\theta_a$ and $\theta_b$. }
\label{fig:shapefromshading}
\end{figure}

\subsubsection{Reflectance Modeling}

We build a simplified bidirectional reflectance model to explain the pixel values, $R_i$, of the IR reflectance image, shown in Figure~\ref{fig:plantnoise}($b$).  This model for pixel $i$ is:
\beq
R_i = \frac{I_i\rho \ray_i\cdot\normal_i s_i}{r_i^2}.\label{eq:reflectanceinit}
\eeq
Here $I_i$ is the intensity of the ray from the IR illuminator assumed to be a point source and decreasing with the inverse square of the distance to target, $r$. Under a Lambertian assumption the reflected beam is decreased by the albedo, $\rho$, and the inner product of the ray direction $\ray_i$ and the normal, $\normal_i$.  Finally the sensor scales the incoming beam with a factor $s_i$.  This model can be simplified further by approximating the outgoing ray $I_i$ as being the same for a given pixel regardless of target depth.  We define a pixel gain $g_i = \log(I_i s_i)$, and the resulting model is:
\beq
R_i = \frac{ \rho \exp(g_i) \ray_i\cdot\normal_i }{r_i^2}. \label{eq:reflectance}
\eeq

The gain values can be calculated from a single depth image of a known surface with constant albedo up to a scale factor (of the unknown albedo).  We observe large gain near the center of the image with tapering towards the edges, along with inter-pixel variations.  We chose to treat the pixel variations as noise and model the gain as a polynomial in normalized pixel coordinates, $u$ and $v$.  That is our modeled gain is:
\beq
g(u,v) = \beta_1 + \beta_2 u + \beta_3 v + \beta_4 u^2 + \beta_5 v^2 \label{eq:gainmodel}
\eeq
Taking the $\log$ of Eq.~(\ref{eq:reflectance}) we can do a least squares fit of the parameters $\beta_i$ and so characterize the gain.  We used a flat target with constant albedo and took data at various inclinations and depths. Figure~\ref{fig:gain} shows the resulting gain model, along with data from one depth image.   

\begin{figure}
\begin{center}
   \includegraphics[trim=100 250 100 250,clip,width=0.95\linewidth]{Figures/gain}
\end{center}
   \caption{Our model for gain $g(u,v)$ in Eq, (\ref{eq:gainmodel}) is fit to a $\log$ reflectance image adjusted with known range and ray angles from Eq.~(\ref{eq:reflectance}).  The dots are the measured data and the surface is the parameterized gain $g(u,v)$. }
\label{fig:gain}
\end{figure}


\subsubsection{Mesh Normals Estimation}

Given a reflectance image providing, $R_i$ for each pixel, a depth image from which we can calculate range to each pixel, $r_i$, and our gain model, $g(u,v)$, we can rearrange and Eq.~({eq:reflectance}) to obtain an inclination prediction, $\eta_i$ at each pixel:
\beq
\eta_i \equiv \rho_t\cos(\theta_i) = R_ir_i^2\exp(-g(u,v)). \label{eq:costheta}
\eeq
The scale factor is the unknown target albedo, $\rho_t$.  If the target such as a leaf has a uniform albedo, this is the same for all target pixels.  

Now each mesh facet has a unit normal which can be encoded in terms of the depths of its three vertices: $\normal(\lambda_a,\lambda_b,\lambda_c)$.  The inner product of this normal with the unit ray to the illuminator is $\cos(\theta)$ and should agree with the values predicted in Eq. (\ref{eq:costheta}) for each reflectance pixel it contains.  This leads to a Shape from Shading cost of
\beq
E_{SfS}(\vlambda_v, \rho_t) = \sum_{i\in \mathcal{T}}\| \frac{\eta_i}{\rho_t} - \normal_i\cdot\ray_i \|^2.
\label{eq:esfs}
\eeq
The sum is over all target pixels, $\mathcal{T}$, projecting into the image mesh, and $\normal_i$ indicates the normal of the facet into whose image the depth pixel projects.  This cost introduces the unknown target albedo, $\rho_t$, as an addition parameter to be optimized over.  The cost is nonlinear, but given a good initial parameters from the least squares solution to Eq.~(\ref{eq:meshleastsquares}), it is readily minimized as a function of vertex depths and albedo.

\subsection{Regularization}

Prior models on surface properties can be incorporated into the mesh via regularization and in so doing reduce the impact of noise.  Membrane energy is a well-used function in mesh optimization~\cite{Kobbelt:1998} and can be minimized using the discrete Laplacian operator.  Here we use Laplacian smoothing due to its simplicity and good performance~\cite{Kobbelt:1998,Ohtake2001789,Chen2005376}, although we modify it to accommodate image-based edge information.  In addition, rather than apply Laplacian smoothing after the fact, entailing an iterative optimization~\cite{Kobbelt:1998}, we show that Laplacian smoothing can be incorporated directly into the least squares mesh estimation.  This has a number of  advantages.  First the smoothing penalty is traded off against measurement error rather than vertex offset.  Second, the due to our ray constraints on the vertices we are able to derive a linear least squares solution to the Laplacian avoiding the need to iterate as in methods such as~\cite{Kobbelt:1998}.  Finally  when the regularization components are added to Eq.~(\ref{eq:meshleastsquares}) they ensure that the solution is well-posed even when some of the facets have no depth points in them.

Laplacian smoothing uses an umbrella-operator~\cite{Kobbelt:1998} on a vertex, $\vertex$, and its neighbors $\vertex_i\in\mathcal{N}(\vertex)$,
\beq
\vect{u}(\vertex) = \frac{1}{n}\sum_{\vertex_i\in\mathcal{N}} \vertex_i - \vertex,\label{eq:umbrella}
\eeq
for $n$ neighbors as illustrated in Figure~\ref{fig:laplacian}.  In our model the vertices lie along known rays and so this operator can be expressed as a function of the vertex depth: $\vect{u}(\lambda) = \frac{1}{n}\sum_{i\in\mathcal{N}} \lambda_i \hvertex_i - \lambda\hvertex$.  The squared magnitude $\|\vect{u}(\lambda)\|^2$ is a natural penalty term as it captures a discrete form of the membrane energy.  Summing this over all vertices and arranging the known $\hvertex$ components into a single matrix $U$, we obtain
\beq
E_{reg}(\vlambda_v) = \| U \vlambda \|^2. \label{eq:reg}
\eeq

\begin{figure}
\begin{center}
   \includegraphics[trim=100 110 140 90,clip,width=0.8\linewidth]{Figures/LaplacianFacets}
\end{center}
   \caption{In discrete form the Laplacing is implemented as an umbrella operator, Eq.~(\ref{eq:umbrella}), over a vertex $\vertex$ and its first neighbors.}
\label{fig:laplacian}
\end{figure}

The cost in Eq. (\ref{eq:reg}) penalizes high curvature regions, and so provides a way incorporate smoothness priors into the mesh estimation.  Now we may have image cues for creases or sharp edges on portions of a mesh, as we describe in section \ref{sec:colormesh}.  We can modify the umbrella operator from Eq. (\ref{eq:umbrella}) so that the neighbors are only other vertices on the crease.  In this way the Laplacian acts along the crease, and not across it, allowing sharper folding along these edges.
 
 
 
