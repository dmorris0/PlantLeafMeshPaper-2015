\section{Mesh Fitting}
\label{sec:fitting}

We pose mesh fitting to 3D point data as finding the most likely surface that would have generated those points.  By incorporating prior surface assumptions, the fitting process estimates a continuous surface from discrete points that can eliminate much of the measurement noise.  Methods that fit mesh models to 3D points often minimize the perpendicular distance of points to facets [REF].  This makes sense when point-cloud noise is equal in all directions or else the point noise is small compared to the facets.  For our data the measurement noise is large and is not equal in all directions, but rather is along the depth camera rays.  Hence the focus of this section is to develop a mesh fitting method that minimizes these pixel depth errors along the pixel rays.

In this paper we define a mesh in a $2$D image space and project it into $3$D.  This is more limiting than full $3$D meshes as it models only the surface portions visible from the sensor, but it also provides a number of advantages.  Compared to methods that fit prior surface models to depth maps [ref], need to search of the space of poses, scales and distortions of the model with the chance of finding local minima.  Compared to voxel-based models with implicit surfaces [ref], our method can better incorporate pixels uncertainties and surface priors, as well as having fewer discretization artifacts.  In addition our method can naturally incorporate detailed features from the high-resolution color camera, and reflectivity information from the IR reflectance image.

\subsection{Notation}

A point, $\point$, is a vector in $3$D.  In a given camera coordinate system, it projects onto a pixel on the unit focal-length image-plane $\hpoint=(u,v,1)^\top$, where the `` $\hat{\text{  }}$ '' indicates a homogeneous vector, and $u$ and $v$ are the coordinates in this plane.  Now $\hpoint$ defines a ray from the camera origin, and the original point is obtained by scaling the image-plane point by its depth, $\lambda$, along the ray, namely: $\point = \lambda\hpoint$. 

\subsection{Facet Model}

Mesh fitting for an individual facet is illustrated in Figure~\ref{fig:facet}.  The $2$D vertices and edge connections are determined in an image, in this case the color image although it could be the depth image, as described in section~\ref{sec:mesh}.  If these vertices lie on a feature of the target leaf, such as its edge, we know that that those $3$D features like somewhere along the rays emanating from camera origin through those vertices.  Hence a triangular facet approximation to the object surface will have vertices on these three rays.  

\begin{figure}
\begin{center}
   \includegraphics[trim=80 70 70 20,clip,width=0.95\linewidth]{Figures/pointFittingConcept}
\end{center}
   \caption{The parallel and adjacent color and depth cameras are shown as pyramids denoting their fields of view, and their size difference illustrates their relative resolutions.  Three vertices in a color image define the rays on which the vertices of the corresponding $3$D object facet must lie.  This facet is fit using the the $3$D points projected out from the depth camera.}
\label{fig:facet}
\end{figure}

The next step is to associate depth measurements with the facet.  Pixels in the depth camera are projected along their rays out into $3$D.  The resulting $3$D points that lie within the facet pyramid (defined by these rays through its vertices) will project into the $2$D facet in the color image.  Hence it is straightforward to associate $3$D points with mesh facets.  

To estimate the facet parameters from depth measurements we will express the depth points as a linear function of the vertices of its facet.  We make a local orthographic approximation for the projection of a facet.  This will be a good approximation as long as the facet size is small compared to is depth from the camera, which is true for most applications.  Given this assumption, the coordinates of a point $\point$ lying on a facet can be expressed as a linear combination of the three vertex coordinates $\vertex_i$, $\vertex_j$ and $\vertex_k$ as follows:
\beq  %see PlantMacros.tex
\point = \alpha_i \vertex_i + \alpha_j \vertex_j + \alpha_k \vertex_k. \label{eq:point}
\eeq
The coefficients $\alpha_i$, $\alpha_j$ and $\alpha_k$ are defined in Figure~\ref{fig:triangle}.  This equation applies both to the image coordinates of the point and vertices, and the $3$D point and $3$D vertices. The coefficients can be computed from this equation in image coordinates as the point and vertices image coordinates are known.  In $3$D we can select the third component of this equation and write it:
\beq
\lambda_{p} = \alpha_i \lambda_i + \alpha_j \lambda_j + \alpha_k \lambda_k, \label{eq:pointdepth}
\eeq
where $\lambda_i$ is the third component of $\vertex_i$ and so forth.  

\begin{figure}
\begin{center}
   \includegraphics[trim=150 140 140 80,clip,width=0.75\linewidth]{Figures/TriangleParameterization}
\end{center}
   \caption{The coordinates of a point on a facet described by Eq.~(\ref{eq:point}) are the weighted linear sum of the three vertex coordinates.  The weight $\alpha_i$ for vertex $\vertex_i$ is the ratio of its perpendicular distance $l_i$ to the opposite edge to the vertex perpendicular distance $h_i$.  Analogous expressions describe $\alpha_j$ and $\alpha_k$. }
\label{fig:triangle}
\end{figure}

\subsection{Least Squares}

Equation (\ref{eq:pointdepth}) gives the depth of one point in terms of its facet vertices.  For mesh with many facets and a measurement with many depth points, a vector of pixel depths, $\vlambda_d$, and vector of vertex depths, $\vlambda_v$, are related with a coefficient matrix, $A$, containing the appropriate $\alpha$'s:
\beq
\vlambda_d = A \vlambda_v. \label{eq:linearmesh}
\eeq
Given a measurement vector of point depths, $\tilde{\vlambda}_d$, expressed in the color camera coordinates, an error vector between these depths and the corresponding mesh points is: $\tilde{\vlambda}_d - A \vlambda_v$.  Notice that this error is along the color camera rays which are almost parallel to the depth camera rays, and thus to a good approximation the noise model in Eq.~(\ref{eq:sigma}) applies.  This leads to the following weighted squared error formulation:
\beq
E_{depth} = \| W \tilde{\vlambda}_d - W A \vlambda_v \|^2, \label{eq:meshleastsquares}
\eeq
where $W$ is a diagonal matrix containing the inverse standard deviation, $\sigma^{-1}$, from Eq.~(\ref{eq:sigma}).

\subsection{Regularization}

Prior models on surface properties can be incorporated into the mesh via regularization and in so doing reduce the impact of noise.  Membrane energy is a well-used function in mesh optimization~\cite{Kobbelt:1998} and can be minimized using the discrete Laplacian operator.  Here we use Laplacian smoothing due to its simplicity and good performance~\cite{Kobbelt:1998,Ohtake2001789,Chen2005376}, although we modify it to accommodate image-based edge information.  In addition, rather than apply Laplacian smoothing after the fact, entailing an iterative optimization~\cite{Kobbelt:1998}, we show that Laplacian smoothing can be incorporated directly into the least squares mesh estimation.  This has a number of  advantages over application after the initial mesh estimation.  First the smoothing penalty is traded off against measurement error rather than vertex offset.  Second, the due to our ray constraints on the vertices we are able to derive a linear solution with not need to iterate.  Finally  when the regularization components are added to Eq.~(\ref{eq:meshleastsquares}) they ensure that the solution is well-posed even when some of the facets have no depth points in them.

Laplacian smoothing uses an umbrella-operator~\cite{Kobbelt:1998} on a vertex, $\vertex$, and its neighbors $\vertex_i\in\mathcal{N}(\vertex)$,
\beq
\vect{u}(\vertex) = \frac{1}{n}\sum_{\vertex_i\in\mathcal{N}} \vertex_i - \vertex,\label{eq:umbrella}
\eeq
for $n$ neighbors as illustrated in Figure~\ref{fig:laplacian}.  In our model the vertices lie along known rays and so this operator can be expressed as a function of the vertex depth: $\vect{u}(\lambda) = \frac{1}{n}\sum_{i\in\mathcal{N}} \lambda_i \hvertex_i - \lambda\hvertex$.  The squared magnitude $\|\vect{u}(\lambda)\|^2$ is a natural penalty term as it captures a discrete form of the membrane energy.  Summing this over all vertices and arranging the known $\hvertex$ components into a single matrix $U$, we obtain
\beq
E_{reg} = \| U \vlambda \|^2.
\eeq

\begin{figure}
\begin{center}
   \includegraphics[trim=100 110 140 90,clip,width=0.9\linewidth]{Figures/LaplacianFacets}
\end{center}
   \caption{In discrete form the Laplacing is implemented as an umbrella operator, Eq.~(\ref{eq:umbrella}), over a vertex $\vertex$ and its first neighbors.}
\label{fig:laplacian}
\end{figure}


